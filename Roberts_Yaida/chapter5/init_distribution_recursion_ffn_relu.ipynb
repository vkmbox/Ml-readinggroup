{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import integrate\n",
    "#from common.ffn.ffn_relu import ParametricReLUNet\n",
    "from common.ffn.ffn_tanh import TanhNet\n",
    "from common.ffn.ffn_base import FFNGmetricLogging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Theoretical values\n",
    "***\n",
    "Consider the FFN with input data dimension ${n_0}$, all layers have width ${n}$; the width at l-layer is denoted by ${n_l}$. For preactivation on l-layer $z^{(l)}$ activation function is $σ(z^{(l)})$, or $σ^{(l)}$. Preactivation weights are initialized with a centered normal distribution with variance $1/n$, except in the first layer, where the variance is $1/{n_0}$; bias is constantly zero. As mentioned, the preactivation at layer l for the trainset with points $α_1...α_N∈D$ is $z^{(l)}$. Consider the distribution:\n",
    "\n",
    "$$p(z^{(l)})_{g,v}=\\frac{1}{Z_{g,v}}exp(-\\frac{1}{2}\\sum \\limits _{k=1} ^{n_l}\\sum \\limits _{α_1,α_2∈D}g^{α_1,α_2}_{(l)}z^{(l)}_{k,α_1}z^{(l)}_{k,α_2})(1+\\frac{1}{8}\\sum \\limits _{k_1,k_2=1} ^{n^l}\\sum \\limits _{α_1,α_2,α_3,α_4∈D}v^{(α_1,α_2)(α_3,α_4)}_{(l)}z^{(l)}_{k_1,α_1}z^{(l)}_{k_1,α_2}z^{(l)}_{k_2,α_3}z^{(l)}_{k_2,α_4}) (1)$$\n",
    "\n",
    "Here, $g^{α_1,α_2}_{(l)}$ and $v^{(α_1,α_2)(α_3,α_4)}_{(l)}$ are calculated via $G^{(l)}_{α_1,α_2}$ and $v^{(l)}_{(α_1,α_2)(α_3,α_4)}$:\n",
    "\n",
    "$$G^{(l+1)}_{α_1,α_2}=<σ^{(l)}_{α_1}σ^{(l)}_{α_2}>_{g^{(l)}}+\\frac{1}{8}\\sum \\limits _{β_1,β_2,β_3,β_4∈D}v^{(β_1,β_2)(β_3,β_4)}_{(l)}(<σ^{(l)}_{α_1}σ^{(l)}_{α_2}z^{(l)}_{β_1,β_2}(z^{(l)}_{β_3,β_4}+2ng^{(l)}_{β_3,β_4})>_{g^{(l)}}-2<σ^{(l)}_{α_1}σ^{(l)}_{α_2}>_{g^{(l)}}g^{(l)}_{β_1,β_3}g^{(l)}_{β_2,β_4}) (2)$$\n",
    "\n",
    "Formula (2) also matches (4.61) in [1]. In this formula $σ(z^{(l)})=σ^{(l)}$; $<⋅>_{g^{(l)}}$ means gaussian integral with covariance matrix $g^{(l)}$ for all greek letters variables mentioned inside <⋅>; $z^{(l)}_{β_1,β_2}=z^{(l)}_{β_1}z^{(l)}_{β_2}-g^{(l)}_{β_1,β_2}$\n",
    "\n",
    "$$v^{(l+1)}_{(α_1,α_2)(α_3,α_4)}=\\frac{1}{n}(<σ^{(l)}_{α_1}σ^{(l)}_{α_2}σ^{(l)}_{α_3}σ^{(l)}_{α_4}>_{g^{(l)}}-<σ^{(l)}_{α_1}σ^{(l)}_{α_2}>_{g^{(l)}}<σ^{(l)}_{α_3}σ^{(l)}_{α_4}>_{g^{(l)}})+\\frac{1}{4}\\sum \\limits _{β_1,β_2,β_3,β_4∈D}v^{(β_1,β_2)(β_3,β_4)}_{(l)}<σ^{(l)}_{α_1}σ^{(l)}_{α_2}z^{(l)}_{β_1,β_2}>_{g^{(l)}}<σ^{(l)}_{α_3}σ^{(l)}_{α_4}z^{(l)}_{β_3,β_4}>_{g^{(l)}} (3)$$\n",
    "\n",
    "Formula (3) matches (4.90) in [1] if the subleading $1/n^2$ correction is neglected and the following sumstitutions are applied: $g^{α_1,α_2}=G^{α_1,α_2}+O(1/n), v^{(β_1,β_2)(β_3,β_4)}=\\frac{1}{n}V^{(β_1,β_2)(β_3,β_4)}+O(1/n^2), v_{(β_1,β_2)(β_3,β_4)}=??$\n",
    "Based on previous values, $g^{α_1,α_2}_{(l+1)}$ and $v^{(α_1,α_2)(α_3,α_4)}_{(l+1)}$ can be computed:\n",
    "\n",
    "$$g^{α_1,α_2}_{(l+1)}=G^{α_1,α_2}_{(l+1)}+\\sum \\limits _{β_1,β_2,β_3,β_4∈D}v^{(l+1)}_{(β_1,β_2)(β_3,β_4)}G^{α_1,β_1}_{(l+1)}(G^{β_2,β_3}_{(l+1)}G^{β_4,α_2}_{(l+1)}+\\frac{n}{2}G^{β_2,α_2}_{(l+1)}G^{β_3,β_4}_{(l+1)}) (4)$$\n",
    "$$v^{(α_1,α_2)(α_3,α_4)}_{(l+1)}=\\sum \\limits _{β_1,β_2,β_3,β_4∈D}G^{α_1,β_1}_{(l+1)}G^{α_2,β_2}_{(l+1)}G^{α_3,β_3}_{(l+1)}G^{α_4,β_4}_{(l+1)}v^{(l+1)}_{(β_1,β_2)(β_3,β_4)} (5)$$\n",
    "\n",
    "The theorem is as follows: Suppose $q(z^{(l)})$ is a true preactivation distribution at l-layer in FFN, $σ(x)=O(x^k), k∈N$. Then, for any S∈Ω, \n",
    "\n",
    "$$|\\int_{z^{(l)}∈S}q(z^{(l)})dz-\\int_{z^{(l)}∈S}p(z^{(l)})_{g,v}dz^{(l)}|=O(\\frac{1}{n^{1.49}}) (6)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Case 1: theoretical values for 1-point train set; all α and β are equal to 1. Input width is 3, all layer widths are 100; number of layers is 5. Activation function is hyperbolic tangent.\n",
    "***\n",
    "Creating FFN, initialising trainset and variance coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNet created with n0=5, nk=100, nl=100, l=5, bias_on=False\n"
     ]
    }
   ],
   "source": [
    "'''n0: # dimension of x\n",
    "    nk: # hidden nodes\n",
    "    nl: # dimension of y\n",
    "    l: # number of layers\n",
    "    nd: # number of points in train-set'''\n",
    "n0,nk,nl,l=5,100,100,5\n",
    "nd=4\n",
    "\n",
    "testNet = TanhNet(n0=n0,nk=nk,nl=nl,l=l)\n",
    "testNet.set_log_level(\"info\")\n",
    "xx = np.random.normal(size=(n0, nd)).astype(np.float32)\n",
    "cb, cw = 0, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def gg_to_pos(g1, g2):\n",
    "    return int((g1-1)*g1/2+g2)-1\n",
    "\n",
    "def vv_to_pos(v1, v2, v3, v4):\n",
    "    aa = gg_to_pos(v1, v2)\n",
    "    return int((aa+1)*aa/2 + gg_to_pos(v3, v4))\n",
    "'''\n",
    "\n",
    "'''Array to save g-coeffs for all l-layers: number of g-coeffs per layer x number of layers\n",
    "    number of g-coeffs per layer - number of combinations from nd by 2 with repetition = [(nd+1)nd]/2.\n",
    "    In g^{α_1,α_2} α_1>=α_2\n",
    "'''\n",
    "matrix_gg_top = np.zeros((l, nd,nd))##gg_to_pos(nd,nd)+1))#int(((nd+1)*nd)/2)))\n",
    "matrix_gg_bottom = np.zeros_like(matrix_gg_top)\n",
    "'''Array to save v-coeffs for all l-layers: number of v-coeffs per layer x number of layers\n",
    "    number of v-coeffs per layer - [(N+1)N]/2=nd(nd+1)(nd(nd+1)+2)/8.\n",
    "    In v^{(α_1,α_2),(α_3,α_4)} α_1>=α_2, α_3>=α_4, α_1+α_2>=α_3+α_4\n",
    "'''\n",
    "matrix_vv_top = np.zeros((l, nd,nd,nd,nd)) #vv_to_pos(nd,nd,nd,nd)+1))#int(nd*(nd+1)*(nd*(nd+1)+2)/8)))\n",
    "matrix_vv_bottom = np.zeros_like(matrix_vv_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest = ['']*(gg_to_pos(5, 5)+1)\\nfor ii in range(1, 6):\\n    for jj in range(1, ii+1):\\n        #print(str(ii) +'-'+ str(jj)+':'+str(gg_to_pos(ii, jj)))\\n        test[gg_to_pos(ii, jj)] = str(ii) +'-'+ str(jj)\\n\\nprint(test)\\n\\ntop=5\\ntest = ['']*(vv_to_pos(top, top, top, top)+1)\\n#cnt=0\\nfor i1 in range(1, top+1):\\n    for i2 in range(1, i1+1):\\n        for j1 in range(1, i1+1):\\n            for j2 in range(1, min(i1+i2-j1, j1)+1):\\n                test[vv_to_pos(i1, i2, j1, j2)] = str(i1) +'-'+ str(i2) +'-'+ str(j1) +'-'+ str(j2)\\n                #cnt+=1\\n                #print(str(i1) +'-'+ str(i2) +'-'+ str(j1) +'-'+ str(j2))\\n\\nprint(len(test))\\n\""
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test = ['']*(gg_to_pos(5, 5)+1)\n",
    "for ii in range(1, 6):\n",
    "    for jj in range(1, ii+1):\n",
    "        #print(str(ii) +'-'+ str(jj)+':'+str(gg_to_pos(ii, jj)))\n",
    "        test[gg_to_pos(ii, jj)] = str(ii) +'-'+ str(jj)\n",
    "\n",
    "print(test)\n",
    "\n",
    "top=5\n",
    "test = ['']*(vv_to_pos(top, top, top, top)+1)\n",
    "#cnt=0\n",
    "for i1 in range(1, top+1):\n",
    "    for i2 in range(1, i1+1):\n",
    "        for j1 in range(1, i1+1):\n",
    "            for j2 in range(1, min(i1+i2-j1, j1)+1):\n",
    "                test[vv_to_pos(i1, i2, j1, j2)] = str(i1) +'-'+ str(i2) +'-'+ str(j1) +'-'+ str(j2)\n",
    "                #cnt+=1\n",
    "                #print(str(i1) +'-'+ str(i2) +'-'+ str(j1) +'-'+ str(j2))\n",
    "\n",
    "print(len(test))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preactivation on 1st layer $z^{(1)}$ has Gaussian distribution as a sum of gaussians weights for each neuron:\n",
    "$$z^{(1)}_{k,α}=b^{(1)}_k+\\sum \\limits _{s=1} ^{n_0}w^{(1)}_{k,s}z^{(0)}_{s,α} (7)$$\n",
    "$$E(z^{(1)}_{k,α})=0 (8)$$\n",
    "$$E(z^{(1)}_{k,α}z^{(1)}_{k,β})=\\frac{1}{n^0}\\sum \\limits _{s=1} ^{n_0}z^{(0)}_{s,α}z^{(0)}_{s,β}=G^{(1)}_{α,β} (9)$$\n",
    "When α=β=1,\n",
    "$$E(z^{(1)}_{k,α})^2=\\frac{1}{n_0}\\sum \\limits _{s=1} ^{n_0}(z^{(0)}_{s,α})^2=G^{(1)}_{α,α} (10)$$\n",
    "Value $v^{(1)}=0$, $G^{(1)}=(G_{(1)})^{-1}, g^{α_1,α_2}_{(1)}=G^{α_1,α_2}_{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_gg_bottom = np.zeros((nd,nd))\n",
    "\n",
    "for ii in range(0, nd):\n",
    "    for jj in range(0, ii+1):\n",
    "        value = FFNGmetricLogging.G_xx(xx[:,ii], xx[:,jj], cb, cw)\n",
    "        #coeffs_gg_bottom[0, gg_to_pos(ii+1, jj+1)] = value\n",
    "        matrix_gg_bottom[0, ii, jj] = value\n",
    "        matrix_gg_bottom[0, jj, ii] = value\n",
    "\n",
    "matrix_gg_top[0] = np.linalg.inv(matrix_gg_bottom[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002421068877084027\n",
      "4\n",
      "[[ 1.25610495  0.45728536 -0.32321093  0.03477004]\n",
      " [ 0.45728536  0.24422181  0.05146189  0.15778646]\n",
      " [-0.32321093  0.05146189  0.55259795  0.25450718]\n",
      " [ 0.03477004  0.15778646  0.25450718  0.54296021]]\n",
      "[[ 19.16271649 -41.74127592  12.84739888   4.88095541]\n",
      " [-41.74127592  95.99411722 -27.72159702 -12.22903761]\n",
      " [ 12.84739888 -27.72159702  10.93491545   2.10765656]\n",
      " [  4.88095541 -12.22903761   2.10765656   4.09505484]]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.det(matrix_gg_bottom[0]))\n",
    "print(np.linalg.matrix_rank(matrix_gg_bottom[0]))\n",
    "print(matrix_gg_bottom[0])\n",
    "print(matrix_gg_top[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.47500457]\n",
      "6.4750045682206565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.039261837647488515"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#((np.matrix(X).T*np.matrix(A)).A * Y.T.A).sum(1)\n",
    "args=(1.0, 1.0, 1.0, 1.0)\n",
    "print(np.dot((np.matrix(args)*np.matrix(matrix_gg_top[0])).A, args))\n",
    "print(np.dot(np.dot(args, matrix_gg_top[0]), args))\n",
    "np.exp(-0.5*np.dot(np.dot(args, matrix_gg_top[0]), args))\n",
    "#zz = np.matrix(args)\n",
    "#print(((zz*np.matrix(matrix_gg_top[0])).A * zz.T.A).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preactivation on 2nd and subsequent layers $z^{(l)}$ is count by formulas (2)-(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.9419379629607925, 0.0499984677530676)\n"
     ]
    }
   ],
   "source": [
    "#for ii in range(1, nd+1):\n",
    "#    for jj in range(1, ii+1):\n",
    "current_layer = 0\n",
    "train_index1_zerobased, train_index2_zerobased = 0,0\n",
    "def density(*args):\n",
    "    return math.exp(-0.5*np.dot(np.dot(args, matrix_gg_top[0]), args)) #np.e**\n",
    "    #return np.exp(-0.5*np.dot((np.matrix(args)*np.matrix(matrix_gg_top[0])).A, args))\n",
    "\n",
    "#val = integrate.nquad(density, [[-np.inf, np.inf]]*nd)\n",
    "val = integrate.nquad(density, [[-10, 10]]*nd, opts=dict(epsabs=5e-02, epsrel=5e-02, limit=100))\n",
    "print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.942510205471129"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((2*np.pi)**nd)*np.linalg.det(matrix_gg_bottom[0]))**0.5 #1.9419379629607925"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
